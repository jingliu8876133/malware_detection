from tokenizers import AddedToken, Tokenizer
from tokenizers.implementations.base_tokenizer import BaseTokenizer
from tokenizers.models import WordLevel
from tokenizers.normalizers import NFD, Lowercase, StripAccents, Sequence
from tokenizers.pre_tokenizers import WhitespaceSplit
from tokenizers.trainers import WordLevelTrainer
from typing import Dict, Optional, Union, List, Iterator


class WordLevelTokenizer(BaseTokenizer):
    def __init__(
            self,
            vocab_dict: Dict[int, str],
            normalize_unicode: bool = False,
            lowercase: bool = False,
            strip_accents: bool = False,
    ):
        tokenizer = Tokenizer(WordLevel(vocab=vocab_dict, unk_token='[UNK]'))
        normalizers = []
        unicode_normalizer = NFD()
        if normalize_unicode:
            normalizers += [unicode_normalizer]
        if lowercase:
            normalizers += [Lowercase()]
        if strip_accents:
            normalizers += [StripAccents()]
        if len(normalizers) > 0:
            if len(normalizers) > 1:
                tokenizer.normalizer = Sequence(normalizers)
            else:
                tokenizer.normalizer = normalizers[0]
        tokenizer.pre_tokenizer = WhitespaceSplit()
        parameters = dict(
            model='WordLevel',
            lowercase=lowercase,
            unicode_normalizer=unicode_normalizer,
            strip_accents=strip_accents,
        )
        super().__init__(tokenizer, parameters)

    def train_from_iterator(
            self,
            iterator: Union[Iterator[str], Iterator[Iterator[str]]],
            vocab_size: int = 350,
            min_frequency: int = 2,
            special_tokens: List[Union[str, AddedToken]] = [
                '[BOS]',
                '[PAD]',
                '[EOS]',
                '[UNK]'
            ],
            show_progress: bool = True,
            length: Optional[int] = None,
    ):
        """Train the model using the given iterator"""

        trainer = WordLevelTrainer(
            vocab_size=vocab_size,
            min_frequency=min_frequency,
            special_tokens=special_tokens,
            show_progress=show_progress,
        )
        self._tokenizer.train_from_iterator(
            iterator,
            trainer=trainer,
            length=length,
        )
