import pandas as pd
import numpy as np
from scipy.stats import entropy
from typing import Dict
from functools import reduce


def calculate_summary_stats(sample):
    file_gb = sample.groupby('file_id')
    n_threads = file_gb.tid.nunique().rename('n_threads')
    n_unique_api_calls_per_file = file_gb.api.nunique().rename('n_unique_calls')
    n_api_calls_per_file = file_gb.api.size().rename('n_calls')
    labels = file_gb.label.first()

    api_sizes_stacked = sample.groupby(['file_id', 'api']).size()
    api_counts_per_file = api_sizes_stacked.unstack(level='api').fillna(0).astype(int)
    api_concentration = pd.Series(
        api_counts_per_file.max(axis=1).div(api_counts_per_file.sum(axis=1)),
        name='api_calls_concentration'
    )
    api_call_entropy = pd.Series(entropy(api_counts_per_file, axis=1),
                                 name='api_calls_entropy', index=api_counts_per_file.index)
    thread_sizes_stacked = sample.groupby(['file_id', 'tid']).size()
    thread_sizes = thread_sizes_stacked.unstack('tid').fillna(0).astype(int)
    thread_size_entropy = pd.Series(
        entropy(thread_sizes, axis=1),
        name='thread_size_entropy',
        index=api_counts_per_file.index
    )
    thread_size_concentration = pd.Series(
        thread_sizes.max(axis=1).div(thread_sizes.sum(axis=1)),
        name='thread_size_concentration'
    )

    stats = pd.concat([
        n_threads,
        n_api_calls_per_file,
        np.log10(n_api_calls_per_file).rename('log10_n_api_calls_per_file'),
        n_unique_api_calls_per_file,
        labels,
        api_concentration,
        api_call_entropy,
        thread_size_concentration,
        thread_size_entropy,
    ], axis=1)
    return stats


def get_percentile_buckets(x, bucket_col, precision=2, percentile_step=0.2):
    quantiles = np.quantile(x[bucket_col], np.arange(0, 1.1, percentile_step))
    bkt = pd.cut(x[bucket_col], quantiles, precision=precision, include_lowest=True, right=True)
    return bkt


def quantile_summary(x, y, bucket_col, precision=2, percentile_step=0.2):
    bkt = get_percentile_buckets(
        x,
        bucket_col,
        precision=precision,
        percentile_step=percentile_step
    )
    return pd.concat([
        y.eq(0).rename('normal'),
        y.ne(0).rename('abnormal')
    ], axis=1).groupby(bkt).sum()


def tokenize(calls: str, token_map: Dict[str, int], padding_width: int, truncate_width):
    mapped = np.array([token_map[x] for x in calls.split()], dtype=np.int32)
    padded = np.pad(mapped, (0, padding_width), mode='constant', constant_values=0)
    return padded[:truncate_width]


def count_params(model):
    params_by_layer = []
    trainable_params_by_layer = []
    for p in model.parameters():
        params_by_layer.append(reduce(np.multiply, p.shape))
        if p.requires_grad:
            trainable_params_by_layer.append(reduce(np.multiply, p.shape))
    print('#params in model:', sum(params_by_layer))
    print('#trainable params in model:', sum(trainable_params_by_layer))