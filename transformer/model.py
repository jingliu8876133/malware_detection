import math
import torch
from torch import nn
# from transformers.models.bert.modeling_bert import BertPooler


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_length, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_length, d_model)
        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(1e4) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class BertPooler(nn.Module):
    """
    Copied from transformers.models.bert.modeling_bert
    to avoid introducing the dependency for now.
    """
    def __init__(self, hidden_size: int):
        super().__init__()
        self.dense = nn.Linear(hidden_size, hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output


class Net(nn.Module):
    def __init__(
            self,
            vocab_size,
            d_model,
            max_length,
            nhead=8,
            dim_feedforward=2048,
            num_layers=6,
            dropout=0.1,
            classification_dropout=None,
            n_classes=8,
            padding_idx=0
    ):
        super().__init__()
        self.emb = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)
        assert d_model % nhead == 0
        self.pos_encoder = PositionalEncoding(
            d_model=d_model,
            dropout=dropout,
            max_length=max_length
        )
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=num_layers
        )
        self.d_model = d_model
        self.pooler = BertPooler(d_model)
        self.classifier = nn.Linear(d_model, n_classes)
        self.classification_dropout = classification_dropout
        if classification_dropout is not None:
            self.class_dropout_layer = nn.Dropout(classification_dropout)

    def forward(self, x, src_key_padding_mask):
        x = self.emb(x) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(
            x,
            src_key_padding_mask=src_key_padding_mask,
        )
        x = self.pooler(x)
        if self.classification_dropout is not None:
            x = self.class_dropout_layer(x)
        x = self.classifier(x)
        return x