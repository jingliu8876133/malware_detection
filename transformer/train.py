import numpy as np
import torch
from tqdm.notebook import tqdm


def train(
        dataloader_train,
        dataloader_val,
        model,
        criterion,
        optimizer,
        config,
        scheduler=None,
):
    torch.manual_seed(42)

    train_losses = []
    val_losses = []

    for epoch in tqdm(range(1, config.epochs + 1)):

        model.train()

        loss_train_total = 0
        # allows you to see the progress of the training
        progress_bar = tqdm(dataloader_train,
                            desc=f'Epoch {epoch:1d}',
                            leave=False,
                            disable=False)
        for batch in progress_bar:
            model.zero_grad()

            batch = tuple(b.to(config.device) for b in batch)

            inputs = {'x': batch[0], 'src_key_padding_mask': batch[1]}

            pred = model(**inputs)
            labels = batch[2]

            loss = criterion(pred, labels)
            loss_train_total += loss.item()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        loss_train_avg = loss_train_total / len(dataloader_train)
        tqdm.write(f'Training loss: {loss_train_avg}')
        train_losses.append(loss_train_avg)

        val_loss, predictions, true_vals = evaluate(
            dataloader_val,
            model,
            criterion,
            config
        )
        tqdm.write(f'Validation loss: {val_loss}')
        val_losses.append(val_loss)
        if scheduler is not None:
            scheduler.step()
    return train_losses, val_losses


def evaluate(dataloader_val, model, criterion, config):
    model.eval()

    loss_val_total = 0
    predictions, true_vals = [], []

    for batch in dataloader_val:
        batch = tuple(b.to(config.device) for b in batch)

        inputs = {'x': batch[0],
                  'src_key_padding_mask': batch[1],
                  }

        labels = batch[2]

        with torch.no_grad():
            pred = model(**inputs)

        loss = criterion(pred, labels)
        loss_val_total += loss.item()

        pred = pred.detach().cpu().numpy()
        label_ids = labels.cpu().numpy()
        predictions.append([pred])
        true_vals.append(label_ids)

    # calculate average val loss
    loss_val_avg = loss_val_total / len(dataloader_val)

    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)

    return loss_val_avg, predictions, true_vals
