import numpy as np
import torch
from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader


def build_dataloaders(
        x_train: np.ndarray,
        x_val: np.ndarray,
        y_train: np.ndarray,
        y_val: np.ndarray,
        batch_size: int,
        drop_last: bool = True,
        pad_token_id: int = 0,
        mask_padded_as_false: bool = False,
):
    """

    Parameters
    ----------
    x_train
    x_val
    y_train
    y_val
    batch_size
    drop_last
    pad_token_id
    mask_padded_as_false:
        Sets attention mask to False on the positions of padding tokens.
        This is the convention used by BERT and its derivative models.
    Returns
    -------

    """
    attention_masks_train = x_train == pad_token_id  # tell the model to ignore padding
    attention_masks_val = x_val == pad_token_id
    if mask_padded_as_false:
        attention_masks_train = ~attention_masks_train
        attention_masks_val = ~attention_masks_val
    dataset_train = TensorDataset(
        torch.from_numpy(x_train),
        torch.from_numpy(attention_masks_train),
        torch.from_numpy(y_train.values)
    )
    dataset_val = TensorDataset(
        torch.from_numpy(x_val),
        torch.from_numpy(attention_masks_val),
        torch.from_numpy(y_val.values)
    )
    dataloader_train = DataLoader(
        dataset_train,
        sampler=RandomSampler(dataset_train),
        batch_size=batch_size,
        drop_last=drop_last
    )
    dataloader_val = DataLoader(
        dataset_val,
        sampler=SequentialSampler(dataset_val),
        batch_size=batch_size,
        drop_last=drop_last
    )
    return dataloader_train, dataloader_val
